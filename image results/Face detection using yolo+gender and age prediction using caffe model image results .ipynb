{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a6ec62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3a1357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e2df616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Utk face dataset\n",
    "#downlaod and sampled upto 60 images for testing the algorithm\n",
    "#https://www.kaggle.com/datasets/jangedoo/utkface-new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1581dcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframe to store orginal age and gender of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e722d9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Filename  Age  Gender\n",
      "0    1_0_0_20161219140623097.jpg.chip.jpg    1    Male\n",
      "1    1_0_0_20161219140627985.jpg.chip.jpg    1    Male\n",
      "2    1_0_0_20161219140642920.jpg.chip.jpg    1    Male\n",
      "3    1_0_0_20161219192208688.jpg.chip.jpg    1    Male\n",
      "4    1_0_0_20161219205141196.jpg.chip.jpg    1    Male\n",
      "..                                    ...  ...     ...\n",
      "62  73_1_0_20170110183953439.jpg.chip.jpg   73  Female\n",
      "63  73_1_1_20170120230417585.jpg.chip.jpg   73  Female\n",
      "64  74_0_0_20170120225714392.jpg.chip.jpg   74    Male\n",
      "65  74_0_0_20170120230055728.jpg.chip.jpg   74    Male\n",
      "66  74_0_1_20170113182114430.jpg.chip.jpg   74    Male\n",
      "\n",
      "[67 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the directory containing the image files\n",
    "directory = 'C://Users/gauth/OneDrive/Desktop/DIP PROJECT/yolo_images'\n",
    "\n",
    "# List to store extracted information\n",
    "data = []\n",
    "\n",
    "# Iterate over files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    # Split filename by \".chip\" to get the main part\n",
    "    main_part = filename.split(\".chip\")[0]\n",
    "    \n",
    "    # Split main part by \"_\" to extract age and gender\n",
    "    age, gender, *rest = main_part.split(\"_\")\n",
    "    \n",
    "    # Convert gender to string representation\n",
    "    gender = \"Male\" if gender == \"0\" else \"Female\"\n",
    "    \n",
    "    # Join the rest of the filename back together (if any)\n",
    "    rest = \"_\".join(rest)\n",
    "    \n",
    "    # Append extracted information to the list\n",
    "    data.append({'Filename': filename, 'Age': int(age), 'Gender': gender})\n",
    "\n",
    "# Create DataFrame from the extracted information\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a78208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb247b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#face detection using yolo+predictions using caffe model on utk face images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a99b1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "\n",
      "0: 640x640 2 faces, 408.2ms\n",
      "Speed: 70.8ms preprocess, 408.2ms inference, 20.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 230.9ms\n",
      "Speed: 21.2ms preprocess, 230.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 172.5ms\n",
      "Speed: 3.4ms preprocess, 172.5ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 177.6ms\n",
      "Speed: 10.5ms preprocess, 177.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 154.4ms\n",
      "Speed: 4.8ms preprocess, 154.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 faces, 141.1ms\n",
      "Speed: 9.1ms preprocess, 141.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 132.1ms\n",
      "Speed: 6.9ms preprocess, 132.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 122.2ms\n",
      "Speed: 7.7ms preprocess, 122.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 120.3ms\n",
      "Speed: 4.5ms preprocess, 120.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 124.4ms\n",
      "Speed: 10.3ms preprocess, 124.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 faces, 121.7ms\n",
      "Speed: 7.4ms preprocess, 121.7ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 126.8ms\n",
      "Speed: 6.5ms preprocess, 126.8ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 120.0ms\n",
      "Speed: 4.3ms preprocess, 120.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 112.5ms\n",
      "Speed: 0.0ms preprocess, 112.5ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 127.2ms\n",
      "Speed: 5.8ms preprocess, 127.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 119.8ms\n",
      "Speed: 5.1ms preprocess, 119.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 136.3ms\n",
      "Speed: 2.8ms preprocess, 136.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 139.8ms\n",
      "Speed: 7.5ms preprocess, 139.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 130.5ms\n",
      "Speed: 9.7ms preprocess, 130.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 134.8ms\n",
      "Speed: 9.5ms preprocess, 134.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 133.9ms\n",
      "Speed: 3.7ms preprocess, 133.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 faces, 173.6ms\n",
      "Speed: 15.6ms preprocess, 173.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 148.9ms\n",
      "Speed: 18.0ms preprocess, 148.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 166.4ms\n",
      "Speed: 15.5ms preprocess, 166.4ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 181.7ms\n",
      "Speed: 0.0ms preprocess, 181.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 154.4ms\n",
      "Speed: 16.5ms preprocess, 154.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 153.1ms\n",
      "Speed: 15.5ms preprocess, 153.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 158.8ms\n",
      "Speed: 8.3ms preprocess, 158.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 158.5ms\n",
      "Speed: 0.0ms preprocess, 158.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 169.1ms\n",
      "Speed: 15.7ms preprocess, 169.1ms inference, 6.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 136.5ms\n",
      "Speed: 13.1ms preprocess, 136.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 152.8ms\n",
      "Speed: 16.2ms preprocess, 152.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 faces, 184.2ms\n",
      "Speed: 31.3ms preprocess, 184.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 135.0ms\n",
      "Speed: 6.1ms preprocess, 135.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 154.6ms\n",
      "Speed: 8.2ms preprocess, 154.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 153.4ms\n",
      "Speed: 17.2ms preprocess, 153.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 135.6ms\n",
      "Speed: 15.6ms preprocess, 135.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 197.8ms\n",
      "Speed: 0.0ms preprocess, 197.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 103.9ms\n",
      "Speed: 14.3ms preprocess, 103.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 158.9ms\n",
      "Speed: 15.7ms preprocess, 158.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 176.4ms\n",
      "Speed: 16.0ms preprocess, 176.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 173.8ms\n",
      "Speed: 15.6ms preprocess, 173.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 167.9ms\n",
      "Speed: 15.4ms preprocess, 167.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 146.1ms\n",
      "Speed: 8.4ms preprocess, 146.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 164.3ms\n",
      "Speed: 15.6ms preprocess, 164.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 140.9ms\n",
      "Speed: 0.0ms preprocess, 140.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 143.0ms\n",
      "Speed: 17.0ms preprocess, 143.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 116.8ms\n",
      "Speed: 4.9ms preprocess, 116.8ms inference, 7.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 184.7ms\n",
      "Speed: 12.4ms preprocess, 184.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 170.4ms\n",
      "Speed: 15.4ms preprocess, 170.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 140.7ms\n",
      "Speed: 4.3ms preprocess, 140.7ms inference, 7.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 153.3ms\n",
      "Speed: 8.5ms preprocess, 153.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 162.4ms\n",
      "Speed: 13.2ms preprocess, 162.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 170.4ms\n",
      "Speed: 8.5ms preprocess, 170.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 169.9ms\n",
      "Speed: 6.5ms preprocess, 169.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 151.6ms\n",
      "Speed: 12.7ms preprocess, 151.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 127.5ms\n",
      "Speed: 7.5ms preprocess, 127.5ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 140.0ms\n",
      "Speed: 7.5ms preprocess, 140.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 135.2ms\n",
      "Speed: 9.5ms preprocess, 135.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 157.2ms\n",
      "Speed: 9.8ms preprocess, 157.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 151.4ms\n",
      "Speed: 15.6ms preprocess, 151.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 150.2ms\n",
      "Speed: 10.4ms preprocess, 150.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 127.1ms\n",
      "Speed: 8.6ms preprocess, 127.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 170.6ms\n",
      "Speed: 15.8ms preprocess, 170.6ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 169.1ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 5.5ms preprocess, 169.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 faces, 148.3ms\n",
      "Speed: 15.6ms preprocess, 148.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 140.2ms\n",
      "Speed: 8.0ms preprocess, 140.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "ageProto = \"age_deploy.prototxt\"\n",
    "ageModel = \"age_net.caffemodel\"\n",
    "\n",
    "genderProto = \"gender_deploy.prototxt\"\n",
    "genderModel = \"gender_net.caffemodel\"\n",
    "\n",
    "ageNet = cv2.dnn.readNet(ageModel, ageProto)\n",
    "genderNet = cv2.dnn.readNet(genderModel, genderProto)\n",
    "\n",
    "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
    "ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
    "genderList = ['Male', 'Female']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "folder_path = 'C://Users/gauth/OneDrive/Desktop/DIP PROJECT/yolo_images'\n",
    "\n",
    "# List all image files in the folder\n",
    "\n",
    "image_files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) ]\n",
    "print(len(image_files))\n",
    "# Iterate over each image\n",
    "result_list=[]\n",
    "model=YOLO(\"yolov8n-face.pt\")\n",
    "for image_file in image_files:\n",
    "    # Read image\n",
    "    img = cv2.imread(image_file)\n",
    "    results=model(img)\n",
    "    result_list.append(results)\n",
    "    boxes=results[0].boxes\n",
    "    \n",
    "    \n",
    "     \n",
    "    #looping over the faces detected from yolo\n",
    "    for box in boxes:\n",
    "        top_left_x=int(box.xyxy.tolist()[0][0])\n",
    "        top_left_y=int(box.xyxy.tolist()[0][1])\n",
    "        bottom_right_x=int(box.xyxy.tolist()[0][2])\n",
    "        bottom_right_y=int(box.xyxy.tolist()[0][3])\n",
    "        \n",
    "        cv2.rectangle(img,( top_left_x,top_left_y),(bottom_right_x,  bottom_right_y),(255,0,0),2)\n",
    "        detected_face=img[top_left_y:bottom_right_y,  top_left_x:bottom_right_x]\n",
    "        \n",
    "        detected_face_blob = cv2.dnn.blobFromImage(detected_face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n",
    "       \n",
    "        \n",
    "        genderNet.setInput(detected_face_blob)\n",
    "        ageNet.setInput(detected_face_blob)\n",
    "        \n",
    "        genderPreds = genderNet.forward()\n",
    "       \n",
    "        \n",
    "        gender = genderList[genderPreds[0].argmax()]\n",
    "        \n",
    "        \n",
    "        agePreds=ageNet.forward()\n",
    "        \n",
    "        age = ageList[agePreds[0].argmax()]\n",
    "        \n",
    "        \n",
    "        label = \"{}{}\".format(gender,age)\n",
    "       \n",
    "        cv2.putText(img, label, (top_left_x+30, top_left_y+30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "        \n",
    "        #loading predicted results to the existing df\n",
    "        df.loc[df['Filename'] == os.path.basename(image_file), 'Predicted_Gender'] = gender\n",
    "        df.loc[df['Filename'] == os.path.basename(image_file), 'Predicted_Age'] = age\n",
    "    cv2.imshow(\"ok\",img)\n",
    "   \n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "\n",
    "       \n",
    "\n",
    "cv2.destroyAllWindows()    \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30cb1378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Predicted_Gender</th>\n",
       "      <th>Predicted_Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_0_0_20161219140623097.jpg.chip.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(4-6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_0_0_20161219140627985.jpg.chip.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(4-6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_0_0_20161219140642920.jpg.chip.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(4-6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_0_0_20161219192208688.jpg.chip.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(0-2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_0_0_20161219205141196.jpg.chip.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(0-2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>73_1_0_20170110183953439.jpg.chip.jpg</td>\n",
       "      <td>73</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>73_1_1_20170120230417585.jpg.chip.jpg</td>\n",
       "      <td>73</td>\n",
       "      <td>Female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>74_0_0_20170120225714392.jpg.chip.jpg</td>\n",
       "      <td>74</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>74_0_0_20170120230055728.jpg.chip.jpg</td>\n",
       "      <td>74</td>\n",
       "      <td>Male</td>\n",
       "      <td>Female</td>\n",
       "      <td>(48-53)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>74_0_1_20170113182114430.jpg.chip.jpg</td>\n",
       "      <td>74</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Filename  Age  Gender Predicted_Gender  \\\n",
       "0    1_0_0_20161219140623097.jpg.chip.jpg    1    Male             Male   \n",
       "1    1_0_0_20161219140627985.jpg.chip.jpg    1    Male             Male   \n",
       "2    1_0_0_20161219140642920.jpg.chip.jpg    1    Male             Male   \n",
       "3    1_0_0_20161219192208688.jpg.chip.jpg    1    Male             Male   \n",
       "4    1_0_0_20161219205141196.jpg.chip.jpg    1    Male             Male   \n",
       "..                                    ...  ...     ...              ...   \n",
       "62  73_1_0_20170110183953439.jpg.chip.jpg   73  Female             Male   \n",
       "63  73_1_1_20170120230417585.jpg.chip.jpg   73  Female              NaN   \n",
       "64  74_0_0_20170120225714392.jpg.chip.jpg   74    Male             Male   \n",
       "65  74_0_0_20170120230055728.jpg.chip.jpg   74    Male           Female   \n",
       "66  74_0_1_20170113182114430.jpg.chip.jpg   74    Male              NaN   \n",
       "\n",
       "   Predicted_Age  \n",
       "0          (4-6)  \n",
       "1          (4-6)  \n",
       "2          (4-6)  \n",
       "3          (0-2)  \n",
       "4          (0-2)  \n",
       "..           ...  \n",
       "62       (25-32)  \n",
       "63           NaN  \n",
       "64       (25-32)  \n",
       "65       (48-53)  \n",
       "66           NaN  \n",
       "\n",
       "[67 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e207d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Predicted_Gender</th>\n",
       "      <th>Predicted_Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>29_0_0_20170117180854265.jpg.chip.jpg</td>\n",
       "      <td>29</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(4-6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>29_0_0_20170117180857403.jpg.chip.jpg</td>\n",
       "      <td>29</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>29_0_0_20170117202637960.jpg.chip.jpg</td>\n",
       "      <td>29</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(38-43)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>29_0_0_20170117202647791.jpg.chip.jpg</td>\n",
       "      <td>29</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>29_0_0_20170117202649479.jpg.chip.jpg</td>\n",
       "      <td>29</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(38-43)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>29_0_0_20170119195119659.jpg.chip.jpg</td>\n",
       "      <td>29</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>31_0_0_20170117133142224.jpg.chip.jpg</td>\n",
       "      <td>31</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(0-2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>31_0_0_20170117133148137.jpg.chip.jpg</td>\n",
       "      <td>31</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(4-6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>31_0_0_20170117135213674.jpg.chip.jpg</td>\n",
       "      <td>31</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(4-6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>31_0_0_20170117135307744.jpg.chip.jpg</td>\n",
       "      <td>31</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(38-43)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>31_0_0_20170117203033455.jpg.chip.jpg</td>\n",
       "      <td>31</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>31_0_0_20170120134310511.jpg.chip.jpg</td>\n",
       "      <td>31</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(4-6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>31_0_0_20170120134346407.jpg.chip.jpg</td>\n",
       "      <td>31</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>32_1_0_20170110143445415.jpg.chip.jpg</td>\n",
       "      <td>32</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>(4-6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>32_1_0_20170111182452820.jpg.chip.jpg</td>\n",
       "      <td>32</td>\n",
       "      <td>Female</td>\n",
       "      <td>Female</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>32_1_1_20170103162943271.jpg.chip.jpg</td>\n",
       "      <td>32</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>32_1_2_20170103181041008.jpg.chip.jpg</td>\n",
       "      <td>32</td>\n",
       "      <td>Female</td>\n",
       "      <td>Female</td>\n",
       "      <td>(38-43)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>32_1_2_20170103183806483.jpg.chip.jpg</td>\n",
       "      <td>32</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>32_1_2_20170104023251558.jpg.chip.jpg</td>\n",
       "      <td>32</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>32_1_2_20170104165057208.jpg.chip.jpg</td>\n",
       "      <td>32</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>32_1_2_20170104165117233.jpg.chip.jpg</td>\n",
       "      <td>32</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>(0-2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>35_0_3_20170104214246948.jpg.chip.jpg</td>\n",
       "      <td>35</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>35_0_3_20170104214512933.jpg.chip.jpg</td>\n",
       "      <td>35</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(38-43)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>35_0_4_20170104201734834.jpg.chip.jpg</td>\n",
       "      <td>35</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>35_0_4_20170109001216009.jpg.chip.jpg</td>\n",
       "      <td>35</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>35_1_0_20170103182736051.jpg.chip.jpg</td>\n",
       "      <td>35</td>\n",
       "      <td>Female</td>\n",
       "      <td>Female</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>35_1_0_20170104201705201.jpg.chip.jpg</td>\n",
       "      <td>35</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>(0-2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>58_0_0_20170117173315296.jpg.chip.jpg</td>\n",
       "      <td>58</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>58_0_0_20170117173317568.jpg.chip.jpg</td>\n",
       "      <td>58</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(0-2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>72_1_0_20170110120829664.jpg.chip.jpg</td>\n",
       "      <td>72</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Filename  Age  Gender Predicted_Gender  \\\n",
       "30  29_0_0_20170117180854265.jpg.chip.jpg   29    Male             Male   \n",
       "31  29_0_0_20170117180857403.jpg.chip.jpg   29    Male             Male   \n",
       "32  29_0_0_20170117202637960.jpg.chip.jpg   29    Male             Male   \n",
       "33  29_0_0_20170117202647791.jpg.chip.jpg   29    Male             Male   \n",
       "34  29_0_0_20170117202649479.jpg.chip.jpg   29    Male             Male   \n",
       "35  29_0_0_20170119195119659.jpg.chip.jpg   29    Male             Male   \n",
       "36  31_0_0_20170117133142224.jpg.chip.jpg   31    Male             Male   \n",
       "37  31_0_0_20170117133148137.jpg.chip.jpg   31    Male             Male   \n",
       "38  31_0_0_20170117135213674.jpg.chip.jpg   31    Male             Male   \n",
       "39  31_0_0_20170117135307744.jpg.chip.jpg   31    Male             Male   \n",
       "40  31_0_0_20170117203033455.jpg.chip.jpg   31    Male             Male   \n",
       "41  31_0_0_20170120134310511.jpg.chip.jpg   31    Male             Male   \n",
       "42  31_0_0_20170120134346407.jpg.chip.jpg   31    Male             Male   \n",
       "43  32_1_0_20170110143445415.jpg.chip.jpg   32  Female             Male   \n",
       "44  32_1_0_20170111182452820.jpg.chip.jpg   32  Female           Female   \n",
       "45  32_1_1_20170103162943271.jpg.chip.jpg   32  Female             Male   \n",
       "46  32_1_2_20170103181041008.jpg.chip.jpg   32  Female           Female   \n",
       "47  32_1_2_20170103183806483.jpg.chip.jpg   32  Female             Male   \n",
       "48  32_1_2_20170104023251558.jpg.chip.jpg   32  Female             Male   \n",
       "49  32_1_2_20170104165057208.jpg.chip.jpg   32  Female             Male   \n",
       "50  32_1_2_20170104165117233.jpg.chip.jpg   32  Female             Male   \n",
       "51  35_0_3_20170104214246948.jpg.chip.jpg   35    Male              NaN   \n",
       "52  35_0_3_20170104214512933.jpg.chip.jpg   35    Male             Male   \n",
       "53  35_0_4_20170104201734834.jpg.chip.jpg   35    Male             Male   \n",
       "54  35_0_4_20170109001216009.jpg.chip.jpg   35    Male             Male   \n",
       "55  35_1_0_20170103182736051.jpg.chip.jpg   35  Female           Female   \n",
       "56  35_1_0_20170104201705201.jpg.chip.jpg   35  Female             Male   \n",
       "57  58_0_0_20170117173315296.jpg.chip.jpg   58    Male             Male   \n",
       "58  58_0_0_20170117173317568.jpg.chip.jpg   58    Male             Male   \n",
       "59  72_1_0_20170110120829664.jpg.chip.jpg   72  Female             Male   \n",
       "\n",
       "   Predicted_Age  \n",
       "30         (4-6)  \n",
       "31       (25-32)  \n",
       "32       (38-43)  \n",
       "33       (25-32)  \n",
       "34       (38-43)  \n",
       "35       (25-32)  \n",
       "36         (0-2)  \n",
       "37         (4-6)  \n",
       "38         (4-6)  \n",
       "39       (38-43)  \n",
       "40       (25-32)  \n",
       "41         (4-6)  \n",
       "42       (25-32)  \n",
       "43         (4-6)  \n",
       "44       (25-32)  \n",
       "45       (25-32)  \n",
       "46       (38-43)  \n",
       "47       (25-32)  \n",
       "48       (25-32)  \n",
       "49       (25-32)  \n",
       "50         (0-2)  \n",
       "51           NaN  \n",
       "52       (38-43)  \n",
       "53       (25-32)  \n",
       "54       (25-32)  \n",
       "55       (25-32)  \n",
       "56         (0-2)  \n",
       "57       (25-32)  \n",
       "58         (0-2)  \n",
       "59       (25-32)  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[30:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5b9fecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for predicted age: 26.87%\n",
      "Accuracy for predicted gender: 59.70%\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy for predicted age\n",
    "correct_age_predictions = 0\n",
    "for index, row in df.iterrows():\n",
    "    actual_age = row['Age']\n",
    "    predicted_age_range = row['Predicted_Age']\n",
    "    if isinstance(predicted_age_range, str):  # Check if the predicted age range is a string\n",
    "        predicted_age_start, predicted_age_end = map(int, predicted_age_range.strip('()').split('-'))  # Extract start and end of predicted age range\n",
    "        if actual_age >= predicted_age_start and actual_age <= predicted_age_end:\n",
    "            correct_age_predictions += 1\n",
    "\n",
    "total_images = df.shape[0]\n",
    "age_accuracy = correct_age_predictions / total_images * 100\n",
    "\n",
    "\n",
    "\n",
    "# Calculate accuracy for predicted gender\n",
    "correct_gender_predictions = (df['Gender'] == df['Predicted_Gender']).sum()\n",
    "gender_accuracy = correct_gender_predictions / total_images * 100\n",
    "\n",
    "# Print results\n",
    "print(\"Accuracy for predicted age: {:.2f}%\".format(age_accuracy))\n",
    "print(\"Accuracy for predicted gender: {:.2f}%\".format(gender_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88eb170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
