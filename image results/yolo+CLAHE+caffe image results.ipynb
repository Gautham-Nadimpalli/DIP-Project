{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f8fe71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2221845e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbec12ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Filename  Age  Gender\n",
      "0    1_0_0_20161219140623097.jpg.chip.jpg    1    Male\n",
      "1    1_0_0_20161219140627985.jpg.chip.jpg    1    Male\n",
      "2    1_0_0_20161219140642920.jpg.chip.jpg    1    Male\n",
      "3    1_0_0_20161219192208688.jpg.chip.jpg    1    Male\n",
      "4    1_0_0_20161219205141196.jpg.chip.jpg    1    Male\n",
      "..                                    ...  ...     ...\n",
      "62  73_1_0_20170110183953439.jpg.chip.jpg   73  Female\n",
      "63  73_1_1_20170120230417585.jpg.chip.jpg   73  Female\n",
      "64  74_0_0_20170120225714392.jpg.chip.jpg   74    Male\n",
      "65  74_0_0_20170120230055728.jpg.chip.jpg   74    Male\n",
      "66  74_0_1_20170113182114430.jpg.chip.jpg   74    Male\n",
      "\n",
      "[67 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the directory containing the image files\n",
    "#change the path\n",
    "directory = 'C://Users/gauth/OneDrive/Desktop/DIP PROJECT/yolo_images'\n",
    "\n",
    "# List to store extracted information\n",
    "data = []\n",
    "\n",
    "# Iterate over files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    # Split filename by \".chip\" to get the main part\n",
    "    main_part = filename.split(\".chip\")[0]\n",
    "    \n",
    "    # Split main part by \"_\" to extract age and gender\n",
    "    age, gender, *rest = main_part.split(\"_\")\n",
    "    \n",
    "    # Convert gender to string representation\n",
    "    gender = \"Male\" if gender == \"0\" else \"Female\"\n",
    "    \n",
    "    # Join the rest of the filename back together (if any)\n",
    "    rest = \"_\".join(rest)\n",
    "    \n",
    "    # Append extracted information to the list\n",
    "    data.append({'Filename': filename, 'Age': int(age), 'Gender': gender})\n",
    "\n",
    "# Create DataFrame from the extracted information\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6c9196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6367711",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5950948a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "\n",
      "0: 640x640 2 faces, 411.2ms\n",
      "Speed: 54.3ms preprocess, 411.2ms inference, 39.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (0-2)\n",
      "gender Male age (8-12)\n",
      "\n",
      "0: 640x640 2 faces, 201.8ms\n",
      "Speed: 15.7ms preprocess, 201.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (25-32)\n",
      "gender Male age (4-6)\n",
      "\n",
      "0: 640x640 1 face, 182.8ms\n",
      "Speed: 7.1ms preprocess, 182.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (25-32)\n",
      "\n",
      "0: 640x640 1 face, 266.5ms\n",
      "Speed: 8.0ms preprocess, 266.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Female age (25-32)\n",
      "\n",
      "0: 640x640 2 faces, 249.0ms\n",
      "Speed: 11.7ms preprocess, 249.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (4-6)\n",
      "gender Female age (0-2)\n",
      "\n",
      "0: 640x640 3 faces, 287.9ms\n",
      "Speed: 9.2ms preprocess, 287.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (0-2)\n",
      "gender Male age (8-12)\n",
      "gender Female age (0-2)\n",
      "\n",
      "0: 640x640 (no detections), 263.8ms\n",
      "Speed: 7.8ms preprocess, 263.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 161.9ms\n",
      "Speed: 11.3ms preprocess, 161.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (8-12)\n",
      "gender Male age (4-6)\n",
      "\n",
      "0: 640x640 2 faces, 126.9ms\n",
      "Speed: 8.0ms preprocess, 126.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (0-2)\n",
      "gender Male age (25-32)\n",
      "\n",
      "0: 640x640 2 faces, 154.7ms\n",
      "Speed: 9.7ms preprocess, 154.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (0-2)\n",
      "gender Male age (4-6)\n",
      "\n",
      "0: 640x640 4 faces, 200.0ms\n",
      "Speed: 15.6ms preprocess, 200.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Female age (0-2)\n",
      "gender Male age (4-6)\n",
      "gender Female age (4-6)\n",
      "gender Male age (0-2)\n",
      "\n",
      "0: 640x640 1 face, 186.4ms\n",
      "Speed: 0.0ms preprocess, 186.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Female age (4-6)\n",
      "\n",
      "0: 640x640 1 face, 172.8ms\n",
      "Speed: 3.3ms preprocess, 172.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (0-2)\n",
      "\n",
      "0: 640x640 (no detections), 161.5ms\n",
      "Speed: 15.6ms preprocess, 161.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 164.7ms\n",
      "Speed: 6.6ms preprocess, 164.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (25-32)\n",
      "\n",
      "0: 640x640 1 face, 160.4ms\n",
      "Speed: 9.6ms preprocess, 160.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (4-6)\n",
      "\n",
      "0: 640x640 1 face, 145.0ms\n",
      "Speed: 8.2ms preprocess, 145.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (38-43)\n",
      "\n",
      "0: 640x640 1 face, 130.2ms\n",
      "Speed: 7.9ms preprocess, 130.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (25-32)\n",
      "\n",
      "0: 640x640 (no detections), 175.1ms\n",
      "Speed: 18.8ms preprocess, 175.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 163.8ms\n",
      "Speed: 0.0ms preprocess, 163.8ms inference, 13.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (0-2)\n",
      "\n",
      "0: 640x640 (no detections), 183.9ms\n",
      "Speed: 20.3ms preprocess, 183.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 faces, 152.5ms\n",
      "Speed: 8.6ms preprocess, 152.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Female age (48-53)\n",
      "gender Female age (25-32)\n",
      "gender Female age (60-100)\n",
      "gender Male age (0-2)\n",
      "\n",
      "0: 640x640 1 face, 134.4ms\n",
      "Speed: 0.0ms preprocess, 134.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (15-20)\n",
      "\n",
      "0: 640x640 2 faces, 130.9ms\n",
      "Speed: 11.7ms preprocess, 130.9ms inference, 14.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (25-32)\n",
      "gender Male age (25-32)\n",
      "\n",
      "0: 640x640 1 face, 150.5ms\n",
      "Speed: 21.0ms preprocess, 150.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (25-32)\n",
      "\n",
      "0: 640x640 2 faces, 144.0ms\n",
      "Speed: 0.0ms preprocess, 144.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Female age (38-43)\n",
      "gender Male age (25-32)\n",
      "\n",
      "0: 640x640 1 face, 160.8ms\n",
      "Speed: 0.0ms preprocess, 160.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (25-32)\n",
      "\n",
      "0: 640x640 1 face, 174.8ms\n",
      "Speed: 15.7ms preprocess, 174.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (48-53)\n",
      "\n",
      "0: 640x640 1 face, 163.6ms\n",
      "Speed: 15.6ms preprocess, 163.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (25-32)\n",
      "\n",
      "0: 640x640 1 face, 163.2ms\n",
      "Speed: 17.2ms preprocess, 163.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (48-53)\n",
      "\n",
      "0: 640x640 2 faces, 150.3ms\n",
      "Speed: 8.1ms preprocess, 150.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (25-32)\n",
      "gender Male age (25-32)\n",
      "\n",
      "0: 640x640 1 face, 168.1ms\n",
      "Speed: 15.7ms preprocess, 168.1ms inference, 15.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (25-32)\n",
      "\n",
      "0: 640x640 3 faces, 121.2ms\n",
      "Speed: 6.6ms preprocess, 121.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (25-32)\n",
      "gender Male age (25-32)\n",
      "gender Male age (38-43)\n",
      "\n",
      "0: 640x640 1 face, 118.1ms\n",
      "Speed: 10.5ms preprocess, 118.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (38-43)\n",
      "\n",
      "0: 640x640 2 faces, 142.7ms\n",
      "Speed: 0.0ms preprocess, 142.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (25-32)\n",
      "gender Male age (38-43)\n",
      "\n",
      "0: 640x640 1 face, 159.1ms\n",
      "Speed: 8.1ms preprocess, 159.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (25-32)\n",
      "\n",
      "0: 640x640 2 faces, 157.6ms\n",
      "Speed: 8.5ms preprocess, 157.6ms inference, 15.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (25-32)\n",
      "gender Male age (25-32)\n",
      "\n",
      "0: 640x640 2 faces, 178.4ms\n",
      "Speed: 0.0ms preprocess, 178.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (25-32)\n",
      "gender Male age (38-43)\n",
      "\n",
      "0: 640x640 1 face, 144.0ms\n",
      "Speed: 17.0ms preprocess, 144.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (25-32)\n",
      "\n",
      "0: 640x640 2 faces, 155.8ms\n",
      "Speed: 0.0ms preprocess, 155.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (25-32)\n",
      "gender Male age (38-43)\n",
      "\n",
      "0: 640x640 1 face, 144.5ms\n",
      "Speed: 15.6ms preprocess, 144.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (25-32)\n",
      "\n",
      "0: 640x640 1 face, 146.8ms\n",
      "Speed: 6.4ms preprocess, 146.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (25-32)\n",
      "\n",
      "0: 640x640 2 faces, 155.6ms\n",
      "Speed: 8.5ms preprocess, 155.6ms inference, 15.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (25-32)\n",
      "gender Male age (25-32)\n",
      "\n",
      "0: 640x640 1 face, 164.5ms\n",
      "Speed: 15.7ms preprocess, 164.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (25-32)\n",
      "\n",
      "0: 640x640 2 faces, 163.0ms\n",
      "Speed: 10.4ms preprocess, 163.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (25-32)\n",
      "gender Male age (4-6)\n",
      "\n",
      "0: 640x640 1 face, 132.5ms\n",
      "Speed: 14.8ms preprocess, 132.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (38-43)\n",
      "\n",
      "0: 640x640 2 faces, 144.1ms\n",
      "Speed: 10.0ms preprocess, 144.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (25-32)\n",
      "gender Female age (25-32)\n",
      "\n",
      "0: 640x640 1 face, 171.9ms\n",
      "Speed: 8.1ms preprocess, 171.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (25-32)\n",
      "\n",
      "0: 640x640 2 faces, 148.8ms\n",
      "Speed: 10.6ms preprocess, 148.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (25-32)\n",
      "gender Male age (4-6)\n",
      "\n",
      "0: 640x640 1 face, 139.8ms\n",
      "Speed: 0.0ms preprocess, 139.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (25-32)\n",
      "\n",
      "0: 640x640 1 face, 148.4ms\n",
      "Speed: 13.1ms preprocess, 148.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender Male age (25-32)\n",
      "\n",
      "0: 640x640 (no detections), 154.8ms\n",
      "Speed: 8.4ms preprocess, 154.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 136.8ms\n",
      "Speed: 23.4ms preprocess, 136.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (4-6)\n",
      "gender Male age (25-32)\n",
      "\n",
      "0: 640x640 2 faces, 144.0ms\n",
      "Speed: 15.7ms preprocess, 144.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (25-32)\n",
      "gender Male age (4-6)\n",
      "\n",
      "0: 640x640 2 faces, 156.3ms\n",
      "Speed: 4.4ms preprocess, 156.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (25-32)\n",
      "gender Female age (25-32)\n",
      "\n",
      "0: 640x640 2 faces, 142.3ms\n",
      "Speed: 7.4ms preprocess, 142.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Female age (38-43)\n",
      "gender Male age (25-32)\n",
      "\n",
      "0: 640x640 2 faces, 158.6ms\n",
      "Speed: 4.2ms preprocess, 158.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (0-2)\n",
      "gender Male age (25-32)\n",
      "\n",
      "0: 640x640 2 faces, 144.1ms\n",
      "Speed: 2.6ms preprocess, 144.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (60-100)\n",
      "gender Male age (25-32)\n",
      "\n",
      "0: 640x640 2 faces, 142.3ms\n",
      "Speed: 18.5ms preprocess, 142.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (60-100)\n",
      "gender Male age (48-53)\n",
      "\n",
      "0: 640x640 1 face, 165.3ms\n",
      "Speed: 15.6ms preprocess, 165.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (25-32)\n",
      "\n",
      "0: 640x640 2 faces, 195.1ms\n",
      "Speed: 0.0ms preprocess, 195.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (38-43)\n",
      "gender Male age (25-32)\n",
      "\n",
      "0: 640x640 2 faces, 174.2ms\n",
      "Speed: 13.2ms preprocess, 174.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Female age (38-43)\n",
      "gender Male age (25-32)\n",
      "\n",
      "0: 640x640 1 face, 160.0ms\n",
      "Speed: 20.7ms preprocess, 160.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (25-32)\n",
      "\n",
      "0: 640x640 (no detections), 141.9ms\n",
      "Speed: 8.8ms preprocess, 141.9ms inference, 9.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 166.5ms\n",
      "Speed: 0.0ms preprocess, 166.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (25-32)\n",
      "\n",
      "0: 640x640 3 faces, 172.5ms\n",
      "Speed: 0.0ms preprocess, 172.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "gender Male age (25-32)\n",
      "gender Male age (4-6)\n",
      "gender Female age (38-43)\n",
      "\n",
      "0: 640x640 (no detections), 157.0ms\n",
      "Speed: 17.3ms preprocess, 157.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "#loading caffe models\n",
    "ageProto = \"age_deploy.prototxt\"\n",
    "ageModel = \"age_net.caffemodel\"\n",
    "\n",
    "genderProto = \"gender_deploy.prototxt\"\n",
    "genderModel = \"gender_net.caffemodel\"\n",
    "\n",
    "ageNet = cv2.dnn.readNet(ageModel, ageProto)\n",
    "genderNet = cv2.dnn.readNet(genderModel, genderProto)\n",
    "\n",
    "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
    "ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
    "genderList = ['Male', 'Female']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#path for images\n",
    "folder_path = 'C://Users/gauth/OneDrive/Desktop/DIP PROJECT/yolo_images'\n",
    "\n",
    "# List all image files in the folder\n",
    "image_files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) ]\n",
    "#No of images\n",
    "print(len(image_files))\n",
    "\n",
    "\n",
    "# Iterate over each image\n",
    "result_list=[]\n",
    "#yolo model being used for face detection\n",
    "model=YOLO(\"yolov8n-face.pt\")\n",
    "for image_file in image_files:\n",
    "    # Read image\n",
    "    img = cv2.imread(image_file)\n",
    "    #getting results yolo model\n",
    "    results=model(img)\n",
    "    result_list.append(results)\n",
    "    boxes=results[0].boxes\n",
    "    \n",
    "    \n",
    "    #looping over the faces detected from yolo\n",
    "    for box in boxes:\n",
    "        top_left_x=int(box.xyxy.tolist()[0][0])\n",
    "        top_left_y=int(box.xyxy.tolist()[0][1])\n",
    "        bottom_right_x=int(box.xyxy.tolist()[0][2])\n",
    "        bottom_right_y=int(box.xyxy.tolist()[0][3])\n",
    "        \n",
    "        cv2.rectangle(img,( top_left_x,top_left_y),(bottom_right_x,  bottom_right_y),(255,0,0),2)\n",
    "        detected_face=img[top_left_y:bottom_right_y,  top_left_x:bottom_right_x]\n",
    "        \n",
    "        #procedure for clahe equalization\n",
    "        lab_img= cv2.cvtColor(detected_face, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "        #Splitting the LAB image to L, A and B channels, respectively\n",
    "        l, a, b = cv2.split(lab_img)\n",
    "\n",
    "      \n",
    "        ########### basic Histogram Equlization#############\n",
    "        #Apply histogram equalization to the L channel\n",
    "        equ = cv2.equalizeHist(l)\n",
    "\n",
    "      \n",
    "        #Combine the Hist. equalized L-channel back with A and B channels\n",
    "        updated_lab_img1 = cv2.merge((equ,a,b))\n",
    "\n",
    "        #Convert LAB image back to color (RGB)\n",
    "        hist_eq_img = cv2.cvtColor(updated_lab_img1, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "        ###########CLAHE#########################\n",
    "        #Apply CLAHE to L channel\n",
    "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "        clahe_img = clahe.apply(l)\n",
    "        \n",
    "        #Combine the CLAHE enhanced L-channel back with A and B channels\n",
    "        updated_lab_img2 = cv2.merge((clahe_img,a,b))\n",
    "\n",
    "        #Convert LAB image back to color (RGB)\n",
    "        CLAHE_img = cv2.cvtColor(updated_lab_img2, cv2.COLOR_LAB2BGR)\n",
    "        \n",
    "        #Normal histogram \n",
    "        cv2.imshow('Basic equlization',hist_eq_img)\n",
    "        \n",
    "        cv2.imshow('CLAHE equlization', CLAHE_img)\n",
    "        \n",
    "        #blob input to caffe\n",
    "        detected_face_blob = cv2.dnn.blobFromImage(CLAHE_img, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n",
    "      \n",
    "        \n",
    "        genderNet.setInput(detected_face_blob)\n",
    "        ageNet.setInput(detected_face_blob)\n",
    "        \n",
    "        genderPreds = genderNet.forward()\n",
    "        gender = genderList[genderPreds[0].argmax()]\n",
    "        \n",
    "        \n",
    "        agePreds=ageNet.forward()\n",
    "        age = ageList[agePreds[0].argmax()]\n",
    "        \n",
    "        print(\"gender\",gender,\"age\",age)\n",
    "        \n",
    "        \n",
    "        label = \"{}{}\".format(gender,age)\n",
    "       \n",
    "        cv2.putText(img, label, (top_left_x+20, top_left_y+20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "      \n",
    "        #loading predicted results to the existing df\n",
    "        df.loc[df['Filename'] == os.path.basename(image_file), 'Predicted_Gender'] = gender\n",
    "        df.loc[df['Filename'] == os.path.basename(image_file), 'Predicted_Age'] = age\n",
    "    cv2.imshow(\"ok\",img)\n",
    "  \n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "\n",
    "       \n",
    "\n",
    "cv2.destroyAllWindows()    \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b354f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Predicted_Gender</th>\n",
       "      <th>Predicted_Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_0_0_20161219140623097.jpg.chip.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(8-12)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_0_0_20161219140627985.jpg.chip.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(4-6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_0_0_20161219140642920.jpg.chip.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_0_0_20161219192208688.jpg.chip.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>Female</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_0_0_20161219205141196.jpg.chip.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>Female</td>\n",
       "      <td>(0-2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>73_1_0_20170110183953439.jpg.chip.jpg</td>\n",
       "      <td>73</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>73_1_1_20170120230417585.jpg.chip.jpg</td>\n",
       "      <td>73</td>\n",
       "      <td>Female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>74_0_0_20170120225714392.jpg.chip.jpg</td>\n",
       "      <td>74</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>74_0_0_20170120230055728.jpg.chip.jpg</td>\n",
       "      <td>74</td>\n",
       "      <td>Male</td>\n",
       "      <td>Female</td>\n",
       "      <td>(38-43)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>74_0_1_20170113182114430.jpg.chip.jpg</td>\n",
       "      <td>74</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Filename  Age  Gender Predicted_Gender  \\\n",
       "0    1_0_0_20161219140623097.jpg.chip.jpg    1    Male             Male   \n",
       "1    1_0_0_20161219140627985.jpg.chip.jpg    1    Male             Male   \n",
       "2    1_0_0_20161219140642920.jpg.chip.jpg    1    Male             Male   \n",
       "3    1_0_0_20161219192208688.jpg.chip.jpg    1    Male           Female   \n",
       "4    1_0_0_20161219205141196.jpg.chip.jpg    1    Male           Female   \n",
       "..                                    ...  ...     ...              ...   \n",
       "62  73_1_0_20170110183953439.jpg.chip.jpg   73  Female             Male   \n",
       "63  73_1_1_20170120230417585.jpg.chip.jpg   73  Female              NaN   \n",
       "64  74_0_0_20170120225714392.jpg.chip.jpg   74    Male             Male   \n",
       "65  74_0_0_20170120230055728.jpg.chip.jpg   74    Male           Female   \n",
       "66  74_0_1_20170113182114430.jpg.chip.jpg   74    Male              NaN   \n",
       "\n",
       "   Predicted_Age  \n",
       "0         (8-12)  \n",
       "1          (4-6)  \n",
       "2        (25-32)  \n",
       "3        (25-32)  \n",
       "4          (0-2)  \n",
       "..           ...  \n",
       "62       (25-32)  \n",
       "63           NaN  \n",
       "64       (25-32)  \n",
       "65       (38-43)  \n",
       "66           NaN  \n",
       "\n",
       "[67 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24e20291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Predicted_Gender</th>\n",
       "      <th>Predicted_Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>31_0_0_20170117203033455.jpg.chip.jpg</td>\n",
       "      <td>31</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>31_0_0_20170120134310511.jpg.chip.jpg</td>\n",
       "      <td>31</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>31_0_0_20170120134346407.jpg.chip.jpg</td>\n",
       "      <td>31</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>32_1_0_20170110143445415.jpg.chip.jpg</td>\n",
       "      <td>32</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>32_1_0_20170111182452820.jpg.chip.jpg</td>\n",
       "      <td>32</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>(4-6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>32_1_1_20170103162943271.jpg.chip.jpg</td>\n",
       "      <td>32</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>(38-43)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>32_1_2_20170103181041008.jpg.chip.jpg</td>\n",
       "      <td>32</td>\n",
       "      <td>Female</td>\n",
       "      <td>Female</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>32_1_2_20170103183806483.jpg.chip.jpg</td>\n",
       "      <td>32</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>32_1_2_20170104023251558.jpg.chip.jpg</td>\n",
       "      <td>32</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>(4-6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>32_1_2_20170104165057208.jpg.chip.jpg</td>\n",
       "      <td>32</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>32_1_2_20170104165117233.jpg.chip.jpg</td>\n",
       "      <td>32</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>35_0_3_20170104214246948.jpg.chip.jpg</td>\n",
       "      <td>35</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>35_0_3_20170104214512933.jpg.chip.jpg</td>\n",
       "      <td>35</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>35_0_4_20170104201734834.jpg.chip.jpg</td>\n",
       "      <td>35</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(4-6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>35_0_4_20170109001216009.jpg.chip.jpg</td>\n",
       "      <td>35</td>\n",
       "      <td>Male</td>\n",
       "      <td>Female</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>35_1_0_20170103182736051.jpg.chip.jpg</td>\n",
       "      <td>35</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>35_1_0_20170104201705201.jpg.chip.jpg</td>\n",
       "      <td>35</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>58_0_0_20170117173315296.jpg.chip.jpg</td>\n",
       "      <td>58</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>58_0_0_20170117173317568.jpg.chip.jpg</td>\n",
       "      <td>58</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(48-53)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>72_1_0_20170110120829664.jpg.chip.jpg</td>\n",
       "      <td>72</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Filename  Age  Gender Predicted_Gender  \\\n",
       "40  31_0_0_20170117203033455.jpg.chip.jpg   31    Male             Male   \n",
       "41  31_0_0_20170120134310511.jpg.chip.jpg   31    Male             Male   \n",
       "42  31_0_0_20170120134346407.jpg.chip.jpg   31    Male             Male   \n",
       "43  32_1_0_20170110143445415.jpg.chip.jpg   32  Female             Male   \n",
       "44  32_1_0_20170111182452820.jpg.chip.jpg   32  Female             Male   \n",
       "45  32_1_1_20170103162943271.jpg.chip.jpg   32  Female             Male   \n",
       "46  32_1_2_20170103181041008.jpg.chip.jpg   32  Female           Female   \n",
       "47  32_1_2_20170103183806483.jpg.chip.jpg   32  Female             Male   \n",
       "48  32_1_2_20170104023251558.jpg.chip.jpg   32  Female             Male   \n",
       "49  32_1_2_20170104165057208.jpg.chip.jpg   32  Female             Male   \n",
       "50  32_1_2_20170104165117233.jpg.chip.jpg   32  Female             Male   \n",
       "51  35_0_3_20170104214246948.jpg.chip.jpg   35    Male              NaN   \n",
       "52  35_0_3_20170104214512933.jpg.chip.jpg   35    Male             Male   \n",
       "53  35_0_4_20170104201734834.jpg.chip.jpg   35    Male             Male   \n",
       "54  35_0_4_20170109001216009.jpg.chip.jpg   35    Male           Female   \n",
       "55  35_1_0_20170103182736051.jpg.chip.jpg   35  Female             Male   \n",
       "56  35_1_0_20170104201705201.jpg.chip.jpg   35  Female             Male   \n",
       "57  58_0_0_20170117173315296.jpg.chip.jpg   58    Male             Male   \n",
       "58  58_0_0_20170117173317568.jpg.chip.jpg   58    Male             Male   \n",
       "59  72_1_0_20170110120829664.jpg.chip.jpg   72  Female             Male   \n",
       "\n",
       "   Predicted_Age  \n",
       "40       (25-32)  \n",
       "41       (25-32)  \n",
       "42       (25-32)  \n",
       "43       (25-32)  \n",
       "44         (4-6)  \n",
       "45       (38-43)  \n",
       "46       (25-32)  \n",
       "47       (25-32)  \n",
       "48         (4-6)  \n",
       "49       (25-32)  \n",
       "50       (25-32)  \n",
       "51           NaN  \n",
       "52       (25-32)  \n",
       "53         (4-6)  \n",
       "54       (25-32)  \n",
       "55       (25-32)  \n",
       "56       (25-32)  \n",
       "57       (25-32)  \n",
       "58       (48-53)  \n",
       "59       (25-32)  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[40:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "147b6475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for predicted age: 32.84%\n",
      "Accuracy for predicted gender: 53.73%\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy for predicted age\n",
    "correct_age_predictions = 0\n",
    "for index, row in df.iterrows():\n",
    "    actual_age = row['Age']\n",
    "    predicted_age_range = row['Predicted_Age']\n",
    "    if isinstance(predicted_age_range, str):  # Check if the predicted age range is a string\n",
    "        predicted_age_start, predicted_age_end = map(int, predicted_age_range.strip('()').split('-'))  # Extract start and end of predicted age range\n",
    "        if actual_age >= predicted_age_start and actual_age <= predicted_age_end:\n",
    "            correct_age_predictions += 1\n",
    "\n",
    "total_images = df.shape[0]\n",
    "age_accuracy = correct_age_predictions / total_images * 100\n",
    "\n",
    "\n",
    "\n",
    "# Calculate accuracy for predicted gender\n",
    "correct_gender_predictions = (df['Gender'] == df['Predicted_Gender']).sum()\n",
    "gender_accuracy = correct_gender_predictions / total_images * 100\n",
    "\n",
    "# Print results\n",
    "print(\"Accuracy for predicted age: {:.2f}%\".format(age_accuracy))\n",
    "print(\"Accuracy for predicted gender: {:.2f}%\".format(gender_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de46ea95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3861fc97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee7ece3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4a3244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4100061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2fdb31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6390ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc50f04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ae954e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af15d19d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8251a8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceba713",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9afb77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0769b8cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d8842a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46bbc6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003f5586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78bb600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b955d19b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b923832b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfdadd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93674b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a111ff97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f6a444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3b05f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe93b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77e12d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4c3e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c06754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e95c1be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372dfcb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e1657b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06db4c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610adb39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f4e5b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e901e2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43d5b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3b8731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfff24a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608dd79a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b707d34b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110943bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c0ffc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a822c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7bba72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c5bfaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df59d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdf5db4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbf4746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
