{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d5f4dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9e503a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6df4c280",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1637529724.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    https://www.kaggle.com/datasets/jangedoo/utkface-new\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#using Utk face dataset\n",
    "#downlaod and sampled upto 60 images for testing the algorithm\n",
    "https://www.kaggle.com/datasets/jangedoo/utkface-new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac53abf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframe to store orginal age and gender of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19f54b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Filename  Age  Gender\n",
      "0    1_0_0_20161219140623097.jpg.chip.jpg    1    Male\n",
      "1    1_0_0_20161219140627985.jpg.chip.jpg    1    Male\n",
      "2    1_0_0_20161219140642920.jpg.chip.jpg    1    Male\n",
      "3    1_0_0_20161219192208688.jpg.chip.jpg    1    Male\n",
      "4    1_0_0_20161219205141196.jpg.chip.jpg    1    Male\n",
      "..                                    ...  ...     ...\n",
      "62  73_1_0_20170110183953439.jpg.chip.jpg   73  Female\n",
      "63  73_1_1_20170120230417585.jpg.chip.jpg   73  Female\n",
      "64  74_0_0_20170120225714392.jpg.chip.jpg   74    Male\n",
      "65  74_0_0_20170120230055728.jpg.chip.jpg   74    Male\n",
      "66  74_0_1_20170113182114430.jpg.chip.jpg   74    Male\n",
      "\n",
      "[67 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the directory containing the image files\n",
    "directory = 'C://Users/gauth/OneDrive/Desktop/DIP PROJECT/yolo_images'\n",
    "\n",
    "# List to store extracted information\n",
    "data = []\n",
    "\n",
    "# Iterate over files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    # Split filename by \".chip\" to get the main part\n",
    "    main_part = filename.split(\".chip\")[0]\n",
    "    \n",
    "    # Split main part by \"_\" to extract age and gender\n",
    "    age, gender, *rest = main_part.split(\"_\")\n",
    "    \n",
    "    # Convert gender to string representation\n",
    "    gender = \"Male\" if gender == \"0\" else \"Female\"\n",
    "    \n",
    "    # Join the rest of the filename back together (if any)\n",
    "    rest = \"_\".join(rest)\n",
    "    \n",
    "    # Append extracted information to the list\n",
    "    data.append({'Filename': filename, 'Age': int(age), 'Gender': gender})\n",
    "\n",
    "# Create DataFrame from the extracted information\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc156ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96608ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#face detection using yolo+predictions using caffe model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4be23a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "\n",
      "0: 640x640 2 faces, 314.5ms\n",
      "Speed: 23.8ms preprocess, 314.5ms inference, 5115.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 101.3ms\n",
      "Speed: 15.4ms preprocess, 101.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 107.5ms\n",
      "Speed: 9.2ms preprocess, 107.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 81.8ms\n",
      "Speed: 12.4ms preprocess, 81.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 61.5ms\n",
      "Speed: 14.4ms preprocess, 61.5ms inference, 15.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 faces, 107.5ms\n",
      "Speed: 8.6ms preprocess, 107.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 96.2ms\n",
      "Speed: 12.6ms preprocess, 96.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 66.9ms\n",
      "Speed: 12.7ms preprocess, 66.9ms inference, 15.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 79.2ms\n",
      "Speed: 14.7ms preprocess, 79.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 111.3ms\n",
      "Speed: 4.4ms preprocess, 111.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 faces, 78.9ms\n",
      "Speed: 19.0ms preprocess, 78.9ms inference, 15.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 74.9ms\n",
      "Speed: 9.7ms preprocess, 74.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 84.4ms\n",
      "Speed: 9.2ms preprocess, 84.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 79.2ms\n",
      "Speed: 9.1ms preprocess, 79.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 111.5ms\n",
      "Speed: 6.7ms preprocess, 111.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 109.0ms\n",
      "Speed: 9.5ms preprocess, 109.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 72.6ms\n",
      "Speed: 7.6ms preprocess, 72.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 84.0ms\n",
      "Speed: 16.0ms preprocess, 84.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 112.4ms\n",
      "Speed: 8.9ms preprocess, 112.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 83.9ms\n",
      "Speed: 18.3ms preprocess, 83.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 77.3ms\n",
      "Speed: 12.4ms preprocess, 77.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 faces, 111.4ms\n",
      "Speed: 19.2ms preprocess, 111.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 87.7ms\n",
      "Speed: 0.0ms preprocess, 87.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 79.4ms\n",
      "Speed: 11.3ms preprocess, 79.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 107.0ms\n",
      "Speed: 19.6ms preprocess, 107.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 77.0ms\n",
      "Speed: 10.2ms preprocess, 77.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 86.4ms\n",
      "Speed: 17.3ms preprocess, 86.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 80.4ms\n",
      "Speed: 5.3ms preprocess, 80.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 77.2ms\n",
      "Speed: 15.8ms preprocess, 77.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 107.1ms\n",
      "Speed: 12.3ms preprocess, 107.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 111.5ms\n",
      "Speed: 9.3ms preprocess, 111.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 104.3ms\n",
      "Speed: 7.4ms preprocess, 104.3ms inference, 15.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 faces, 87.6ms\n",
      "Speed: 15.4ms preprocess, 87.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 80.1ms\n",
      "Speed: 15.5ms preprocess, 80.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 79.2ms\n",
      "Speed: 2.9ms preprocess, 79.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 80.6ms\n",
      "Speed: 6.8ms preprocess, 80.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 69.9ms\n",
      "Speed: 7.9ms preprocess, 69.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 78.4ms\n",
      "Speed: 5.2ms preprocess, 78.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 75.7ms\n",
      "Speed: 10.0ms preprocess, 75.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 72.9ms\n",
      "Speed: 19.8ms preprocess, 72.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 105.4ms\n",
      "Speed: 8.3ms preprocess, 105.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 111.2ms\n",
      "Speed: 9.8ms preprocess, 111.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 108.1ms\n",
      "Speed: 17.8ms preprocess, 108.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 73.6ms\n",
      "Speed: 3.8ms preprocess, 73.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 72.4ms\n",
      "Speed: 19.3ms preprocess, 72.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 104.6ms\n",
      "Speed: 6.0ms preprocess, 104.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 74.3ms\n",
      "Speed: 16.6ms preprocess, 74.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 80.3ms\n",
      "Speed: 20.3ms preprocess, 80.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 74.1ms\n",
      "Speed: 6.7ms preprocess, 74.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 78.0ms\n",
      "Speed: 11.4ms preprocess, 78.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 74.7ms\n",
      "Speed: 6.6ms preprocess, 74.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 94.0ms\n",
      "Speed: 18.4ms preprocess, 94.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 104.3ms\n",
      "Speed: 17.0ms preprocess, 104.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 107.4ms\n",
      "Speed: 20.6ms preprocess, 107.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 74.7ms\n",
      "Speed: 6.5ms preprocess, 74.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 74.7ms\n",
      "Speed: 5.5ms preprocess, 74.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 74.4ms\n",
      "Speed: 16.2ms preprocess, 74.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 90.0ms\n",
      "Speed: 17.3ms preprocess, 90.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 76.1ms\n",
      "Speed: 13.6ms preprocess, 76.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 73.4ms\n",
      "Speed: 7.4ms preprocess, 73.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 73.9ms\n",
      "Speed: 10.0ms preprocess, 73.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 faces, 93.0ms\n",
      "Speed: 5.5ms preprocess, 93.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 74.9ms\n",
      "Speed: 9.7ms preprocess, 74.9ms inference, 15.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 76.0ms\n",
      "Speed: 7.2ms preprocess, 76.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 face, 117.7ms\n",
      "Speed: 10.3ms preprocess, 117.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 3 faces, 106.4ms\n",
      "Speed: 10.7ms preprocess, 106.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 61.8ms\n",
      "Speed: 16.1ms preprocess, 61.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "ageProto = \"age_deploy.prototxt\"\n",
    "ageModel = \"age_net.caffemodel\"\n",
    "\n",
    "genderProto = \"gender_deploy.prototxt\"\n",
    "genderModel = \"gender_net.caffemodel\"\n",
    "\n",
    "ageNet = cv2.dnn.readNet(ageModel, ageProto)\n",
    "genderNet = cv2.dnn.readNet(genderModel, genderProto)\n",
    "\n",
    "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
    "ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
    "genderList = ['Male', 'Female']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "folder_path = 'C://Users/gauth/OneDrive/Desktop/DIP PROJECT/yolo_images'\n",
    "\n",
    "# List all image files in the folder\n",
    "\n",
    "image_files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) ]\n",
    "print(len(image_files))\n",
    "# Iterate over each image\n",
    "result_list=[]\n",
    "model=YOLO(\"yolov8n-face.pt\")\n",
    "for image_file in image_files:\n",
    "    # Read image\n",
    "    img = cv2.imread(image_file)\n",
    "    results=model(img)\n",
    "    result_list.append(results)\n",
    "    boxes=results[0].boxes\n",
    "    \n",
    "    \n",
    "   \n",
    "    for box in boxes:\n",
    "        top_left_x=int(box.xyxy.tolist()[0][0])\n",
    "        top_left_y=int(box.xyxy.tolist()[0][1])\n",
    "        bottom_right_x=int(box.xyxy.tolist()[0][2])\n",
    "        bottom_right_y=int(box.xyxy.tolist()[0][3])\n",
    "        \n",
    "        cv2.rectangle(img,( top_left_x,top_left_y),(bottom_right_x,  bottom_right_y),(255,0,0),2)\n",
    "        detected_face=img[top_left_y:bottom_right_y,  top_left_x:bottom_right_x]\n",
    "        \n",
    "        detected_face_blob = cv2.dnn.blobFromImage(detected_face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n",
    "       \n",
    "        \n",
    "        genderNet.setInput(detected_face_blob)\n",
    "        ageNet.setInput(detected_face_blob)\n",
    "        \n",
    "        genderPreds = genderNet.forward()\n",
    "       \n",
    "        \n",
    "        gender = genderList[genderPreds[0].argmax()]\n",
    "        \n",
    "        \n",
    "        agePreds=ageNet.forward()\n",
    "        \n",
    "        age = ageList[agePreds[0].argmax()]\n",
    "        \n",
    "        \n",
    "        label = \"{}{}\".format(gender,age)\n",
    "       \n",
    "        cv2.putText(img, label, (top_left_x+30, top_left_y+30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "        cv2.imshow(\"faces\",detected_face)\n",
    "        #df = df.append({'Filename': os.path.basename(image_file), 'Predicted Gender': gender, 'Age': age}, ignore_index=True)\n",
    "        df.loc[df['Filename'] == os.path.basename(image_file), 'Predicted_Gender'] = gender\n",
    "        df.loc[df['Filename'] == os.path.basename(image_file), 'Predicted_Age'] = age\n",
    "    cv2.imshow(\"ok\",img)\n",
    "   \n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "\n",
    "       \n",
    "\n",
    "cv2.destroyAllWindows()    \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b263a3ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Predicted_Gender</th>\n",
       "      <th>Predicted_Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_0_0_20161219140623097.jpg.chip.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(4-6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_0_0_20161219140627985.jpg.chip.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(4-6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_0_0_20161219140642920.jpg.chip.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(4-6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_0_0_20161219192208688.jpg.chip.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(0-2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_0_0_20161219205141196.jpg.chip.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(0-2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>73_1_0_20170110183953439.jpg.chip.jpg</td>\n",
       "      <td>73</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>73_1_1_20170120230417585.jpg.chip.jpg</td>\n",
       "      <td>73</td>\n",
       "      <td>Female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>74_0_0_20170120225714392.jpg.chip.jpg</td>\n",
       "      <td>74</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>74_0_0_20170120230055728.jpg.chip.jpg</td>\n",
       "      <td>74</td>\n",
       "      <td>Male</td>\n",
       "      <td>Female</td>\n",
       "      <td>(48-53)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>74_0_1_20170113182114430.jpg.chip.jpg</td>\n",
       "      <td>74</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Filename  Age  Gender Predicted_Gender  \\\n",
       "0    1_0_0_20161219140623097.jpg.chip.jpg    1    Male             Male   \n",
       "1    1_0_0_20161219140627985.jpg.chip.jpg    1    Male             Male   \n",
       "2    1_0_0_20161219140642920.jpg.chip.jpg    1    Male             Male   \n",
       "3    1_0_0_20161219192208688.jpg.chip.jpg    1    Male             Male   \n",
       "4    1_0_0_20161219205141196.jpg.chip.jpg    1    Male             Male   \n",
       "..                                    ...  ...     ...              ...   \n",
       "62  73_1_0_20170110183953439.jpg.chip.jpg   73  Female             Male   \n",
       "63  73_1_1_20170120230417585.jpg.chip.jpg   73  Female              NaN   \n",
       "64  74_0_0_20170120225714392.jpg.chip.jpg   74    Male             Male   \n",
       "65  74_0_0_20170120230055728.jpg.chip.jpg   74    Male           Female   \n",
       "66  74_0_1_20170113182114430.jpg.chip.jpg   74    Male              NaN   \n",
       "\n",
       "   Predicted_Age  \n",
       "0          (4-6)  \n",
       "1          (4-6)  \n",
       "2          (4-6)  \n",
       "3          (0-2)  \n",
       "4          (0-2)  \n",
       "..           ...  \n",
       "62       (25-32)  \n",
       "63           NaN  \n",
       "64       (25-32)  \n",
       "65       (48-53)  \n",
       "66           NaN  \n",
       "\n",
       "[67 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01064afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Predicted_Gender</th>\n",
       "      <th>Predicted_Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>29_0_0_20170117180854265.jpg.chip.jpg</td>\n",
       "      <td>29</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(4-6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>29_0_0_20170117180857403.jpg.chip.jpg</td>\n",
       "      <td>29</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>29_0_0_20170117202637960.jpg.chip.jpg</td>\n",
       "      <td>29</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(38-43)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>29_0_0_20170117202647791.jpg.chip.jpg</td>\n",
       "      <td>29</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>29_0_0_20170117202649479.jpg.chip.jpg</td>\n",
       "      <td>29</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(38-43)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>29_0_0_20170119195119659.jpg.chip.jpg</td>\n",
       "      <td>29</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>31_0_0_20170117133142224.jpg.chip.jpg</td>\n",
       "      <td>31</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(0-2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>31_0_0_20170117133148137.jpg.chip.jpg</td>\n",
       "      <td>31</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(4-6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>31_0_0_20170117135213674.jpg.chip.jpg</td>\n",
       "      <td>31</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(4-6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>31_0_0_20170117135307744.jpg.chip.jpg</td>\n",
       "      <td>31</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(38-43)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>31_0_0_20170117203033455.jpg.chip.jpg</td>\n",
       "      <td>31</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>31_0_0_20170120134310511.jpg.chip.jpg</td>\n",
       "      <td>31</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(4-6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>31_0_0_20170120134346407.jpg.chip.jpg</td>\n",
       "      <td>31</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>32_1_0_20170110143445415.jpg.chip.jpg</td>\n",
       "      <td>32</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>(4-6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>32_1_0_20170111182452820.jpg.chip.jpg</td>\n",
       "      <td>32</td>\n",
       "      <td>Female</td>\n",
       "      <td>Female</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>32_1_1_20170103162943271.jpg.chip.jpg</td>\n",
       "      <td>32</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>32_1_2_20170103181041008.jpg.chip.jpg</td>\n",
       "      <td>32</td>\n",
       "      <td>Female</td>\n",
       "      <td>Female</td>\n",
       "      <td>(38-43)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>32_1_2_20170103183806483.jpg.chip.jpg</td>\n",
       "      <td>32</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>32_1_2_20170104023251558.jpg.chip.jpg</td>\n",
       "      <td>32</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>32_1_2_20170104165057208.jpg.chip.jpg</td>\n",
       "      <td>32</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>32_1_2_20170104165117233.jpg.chip.jpg</td>\n",
       "      <td>32</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>(0-2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>35_0_3_20170104214246948.jpg.chip.jpg</td>\n",
       "      <td>35</td>\n",
       "      <td>Male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>35_0_3_20170104214512933.jpg.chip.jpg</td>\n",
       "      <td>35</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(38-43)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>35_0_4_20170104201734834.jpg.chip.jpg</td>\n",
       "      <td>35</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>35_0_4_20170109001216009.jpg.chip.jpg</td>\n",
       "      <td>35</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>35_1_0_20170103182736051.jpg.chip.jpg</td>\n",
       "      <td>35</td>\n",
       "      <td>Female</td>\n",
       "      <td>Female</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>35_1_0_20170104201705201.jpg.chip.jpg</td>\n",
       "      <td>35</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>(0-2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>58_0_0_20170117173315296.jpg.chip.jpg</td>\n",
       "      <td>58</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>58_0_0_20170117173317568.jpg.chip.jpg</td>\n",
       "      <td>58</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>(0-2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>72_1_0_20170110120829664.jpg.chip.jpg</td>\n",
       "      <td>72</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>(25-32)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Filename  Age  Gender Predicted_Gender  \\\n",
       "30  29_0_0_20170117180854265.jpg.chip.jpg   29    Male             Male   \n",
       "31  29_0_0_20170117180857403.jpg.chip.jpg   29    Male             Male   \n",
       "32  29_0_0_20170117202637960.jpg.chip.jpg   29    Male             Male   \n",
       "33  29_0_0_20170117202647791.jpg.chip.jpg   29    Male             Male   \n",
       "34  29_0_0_20170117202649479.jpg.chip.jpg   29    Male             Male   \n",
       "35  29_0_0_20170119195119659.jpg.chip.jpg   29    Male             Male   \n",
       "36  31_0_0_20170117133142224.jpg.chip.jpg   31    Male             Male   \n",
       "37  31_0_0_20170117133148137.jpg.chip.jpg   31    Male             Male   \n",
       "38  31_0_0_20170117135213674.jpg.chip.jpg   31    Male             Male   \n",
       "39  31_0_0_20170117135307744.jpg.chip.jpg   31    Male             Male   \n",
       "40  31_0_0_20170117203033455.jpg.chip.jpg   31    Male             Male   \n",
       "41  31_0_0_20170120134310511.jpg.chip.jpg   31    Male             Male   \n",
       "42  31_0_0_20170120134346407.jpg.chip.jpg   31    Male             Male   \n",
       "43  32_1_0_20170110143445415.jpg.chip.jpg   32  Female             Male   \n",
       "44  32_1_0_20170111182452820.jpg.chip.jpg   32  Female           Female   \n",
       "45  32_1_1_20170103162943271.jpg.chip.jpg   32  Female             Male   \n",
       "46  32_1_2_20170103181041008.jpg.chip.jpg   32  Female           Female   \n",
       "47  32_1_2_20170103183806483.jpg.chip.jpg   32  Female             Male   \n",
       "48  32_1_2_20170104023251558.jpg.chip.jpg   32  Female             Male   \n",
       "49  32_1_2_20170104165057208.jpg.chip.jpg   32  Female             Male   \n",
       "50  32_1_2_20170104165117233.jpg.chip.jpg   32  Female             Male   \n",
       "51  35_0_3_20170104214246948.jpg.chip.jpg   35    Male              NaN   \n",
       "52  35_0_3_20170104214512933.jpg.chip.jpg   35    Male             Male   \n",
       "53  35_0_4_20170104201734834.jpg.chip.jpg   35    Male             Male   \n",
       "54  35_0_4_20170109001216009.jpg.chip.jpg   35    Male             Male   \n",
       "55  35_1_0_20170103182736051.jpg.chip.jpg   35  Female           Female   \n",
       "56  35_1_0_20170104201705201.jpg.chip.jpg   35  Female             Male   \n",
       "57  58_0_0_20170117173315296.jpg.chip.jpg   58    Male             Male   \n",
       "58  58_0_0_20170117173317568.jpg.chip.jpg   58    Male             Male   \n",
       "59  72_1_0_20170110120829664.jpg.chip.jpg   72  Female             Male   \n",
       "\n",
       "   Predicted_Age  \n",
       "30         (4-6)  \n",
       "31       (25-32)  \n",
       "32       (38-43)  \n",
       "33       (25-32)  \n",
       "34       (38-43)  \n",
       "35       (25-32)  \n",
       "36         (0-2)  \n",
       "37         (4-6)  \n",
       "38         (4-6)  \n",
       "39       (38-43)  \n",
       "40       (25-32)  \n",
       "41         (4-6)  \n",
       "42       (25-32)  \n",
       "43         (4-6)  \n",
       "44       (25-32)  \n",
       "45       (25-32)  \n",
       "46       (38-43)  \n",
       "47       (25-32)  \n",
       "48       (25-32)  \n",
       "49       (25-32)  \n",
       "50         (0-2)  \n",
       "51           NaN  \n",
       "52       (38-43)  \n",
       "53       (25-32)  \n",
       "54       (25-32)  \n",
       "55       (25-32)  \n",
       "56         (0-2)  \n",
       "57       (25-32)  \n",
       "58         (0-2)  \n",
       "59       (25-32)  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[30:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cba22a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for predicted age: 26.87%\n",
      "Accuracy for predicted gender: 59.70%\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy for predicted age\n",
    "correct_age_predictions = 0\n",
    "for index, row in df.iterrows():\n",
    "    actual_age = row['Age']\n",
    "    predicted_age_range = row['Predicted_Age']\n",
    "    if isinstance(predicted_age_range, str):  # Check if the predicted age range is a string\n",
    "        predicted_age_start, predicted_age_end = map(int, predicted_age_range.strip('()').split('-'))  # Extract start and end of predicted age range\n",
    "        if actual_age >= predicted_age_start and actual_age <= predicted_age_end:\n",
    "            correct_age_predictions += 1\n",
    "\n",
    "total_images = df.shape[0]\n",
    "age_accuracy = correct_age_predictions / total_images * 100\n",
    "\n",
    "\n",
    "\n",
    "# Calculate accuracy for predicted gender\n",
    "correct_gender_predictions = (df['Gender'] == df['Predicted_Gender']).sum()\n",
    "gender_accuracy = correct_gender_predictions / total_images * 100\n",
    "\n",
    "# Print results\n",
    "print(\"Accuracy for predicted age: {:.2f}%\".format(age_accuracy))\n",
    "print(\"Accuracy for predicted gender: {:.2f}%\".format(gender_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4174f42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
